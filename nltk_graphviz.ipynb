{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.corpus import brown\n",
    "import numpy as np\n",
    "from graphviz import Digraph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = \"\"\"At eight o'clock on Thursday morning Arthur didn't feel very good.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "dot = Digraph(comment='test')\n",
    "dot.node('A', 'King Arthur')\n",
    "dot.node('B', 'Sir Bedevere the Wise')\n",
    "dot.node('L', 'Sir Lancelot the Brave')\n",
    "dot.edges(['AL'])\n",
    "dot.edges(['LB'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "// test\n",
      "digraph {\n",
      "\tA [label=\"King Arthur\"]\n",
      "\tB [label=\"Sir Bedevere the Wise\"]\n",
      "\tL [label=\"Sir Lancelot the Brave\"]\n",
      "\tA -> L\n",
      "\tL -> B\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "print(dot.source)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Aaron', 'male'), ('Abbey', 'male'), ('Abbie', 'male'), ('Abbot', 'male')]\n",
      "[('Abagail', 'female'), ('Abbe', 'female'), ('Abbey', 'female'), ('Abbi', 'female')]\n"
     ]
    }
   ],
   "source": [
    "names = nltk.corpus.names\n",
    "male_names = [(name, 'male') for name in names.words('male.txt')]\n",
    "female_names = [(name, 'female') for name in names.words('female.txt')]\n",
    "print(male_names[1:5])\n",
    "print(female_names[1:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'test-output\\\\round-table.gv.pdf'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dot.render('test-output/round-table.gv', view=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['sentiment', 'analysis', 'is', 'a', 'challenging', 'subject', 'in', 'ml', '.', 'people', 'express', 'their', 'emotions', 'in', 'language', 'who', 'is', 'often', 'obscured', 'by', 'sarcasm', ',', 'ambiguity', ',', 'and', 'play', 'on', 'words', ',', 'all', 'of', 'which', 'could', 'be', 'very', 'misleading', 'for', 'both', 'humans', 'and', 'computers', '.']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('sentiment', 'NN'),\n",
       " ('analysis', 'NN'),\n",
       " ('is', 'VBZ'),\n",
       " ('a', 'DT'),\n",
       " ('challenging', 'VBG'),\n",
       " ('subject', 'NN'),\n",
       " ('in', 'IN'),\n",
       " ('ml', 'NN'),\n",
       " ('.', '.'),\n",
       " ('people', 'NNS'),\n",
       " ('express', 'VBP'),\n",
       " ('their', 'PRP$'),\n",
       " ('emotions', 'NNS'),\n",
       " ('in', 'IN'),\n",
       " ('language', 'NN'),\n",
       " ('who', 'WP'),\n",
       " ('is', 'VBZ'),\n",
       " ('often', 'RB'),\n",
       " ('obscured', 'VBN'),\n",
       " ('by', 'IN'),\n",
       " ('sarcasm', 'NN'),\n",
       " (',', ','),\n",
       " ('ambiguity', 'NN'),\n",
       " (',', ','),\n",
       " ('and', 'CC'),\n",
       " ('play', 'VB'),\n",
       " ('on', 'IN'),\n",
       " ('words', 'NNS'),\n",
       " (',', ','),\n",
       " ('all', 'DT'),\n",
       " ('of', 'IN'),\n",
       " ('which', 'WDT'),\n",
       " ('could', 'MD'),\n",
       " ('be', 'VB'),\n",
       " ('very', 'RB'),\n",
       " ('misleading', 'VBG'),\n",
       " ('for', 'IN'),\n",
       " ('both', 'DT'),\n",
       " ('humans', 'NNS'),\n",
       " ('and', 'CC'),\n",
       " ('computers', 'NNS'),\n",
       " ('.', '.')]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "text = \"Sentiment analysis is a challenging subject in ML.\\\n",
    " People express their emotions in language who is often obscured by sarcasm,\\\n",
    "  ambiguity, and play on words, all of which could be very misleading for \\\n",
    "  both humans and computers.\".lower()\n",
    "text_list = nltk.word_tokenize(text)\n",
    "print(text_list)\n",
    "english_punctuations = [ ':', ';', '?', '(', ')', '[', ']', '&', '!', '*', '@', '#', '$', '%']\n",
    "text_list = [word for word in text_list if word not in english_punctuations] #去除符號\n",
    "text_word=nltk.pos_tag(text_list)\n",
    "text_word\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "// test\n",
      "digraph {\n",
      "\tA [label=\"sentiment analysis\"]\n",
      "\tB [label=is]\n",
      "\tL [label=subject]\n",
      "\tL [label=ml]\n",
      "\tA -> B\n",
      "\tB -> L\n",
      "}\n",
      "8\n"
     ]
    }
   ],
   "source": [
    "count=0\n",
    "temp=''\n",
    "dot = Digraph(comment='test')\n",
    "for i in range(0,len(text_word)):\n",
    "    if(text_word[i][1]==\"NN\"):\n",
    "        if(count==0):\n",
    "            if(text_word[i+1][1]==\"NN\"):\n",
    "                if(temp==''):\n",
    "                    temp=text_word[i][0]+' ' +text_word[i+1][0]\n",
    "                else:\n",
    "                    temp=temp+' '+text_word[i+1][0]\n",
    "            else :\n",
    "                if(temp==''):\n",
    "                    temp=text_word[i][0]\n",
    "                dot.node('A', temp)\n",
    "                temp=''\n",
    "                count=count+1\n",
    "        elif(count>0):\n",
    "            if(text_word[i+1][1]==\"NN\"):\n",
    "                if(temp==''):\n",
    "                    temp=text_word[i][0]+text_word[i+1][0]\n",
    "                else:\n",
    "                    temp=temp+text_word[i+1][0]\n",
    "            else :\n",
    "                if(temp==''):\n",
    "                    temp=text_word[i][0]\n",
    "                dot.node('L', temp)\n",
    "                temp=''\n",
    "                count=count+1\n",
    "    elif(text_word[i][1]==\"VBZ\"):\n",
    "        dot.node('B', text_word[i][0])\n",
    "        count=count+1\n",
    "    elif(text_word[i][1]=='.'):\n",
    "        dot.edges(['AB'])\n",
    "        dot.edges(['BL'])\n",
    "        dot.render('test-output/'+str(i), view=True)\n",
    "        print(dot.source)\n",
    "        print(i)\n",
    "        break\n",
    "        count=0\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dot = Digraph(comment='test')\n",
    "dot.node('A', 'King Arthur')\n",
    "dot.node('B', 'Sir Bedevere the Wise')\n",
    "dot.node('L', 'Sir Lancelot the Brave')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'test-output\\\\round-table.gv.pdf'"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dot.render('test-output/round-table.gv', view=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'int'>\n",
      "<class 'int'>\n",
      "<class 'int'>\n",
      "<class 'int'>\n",
      "<class 'int'>\n",
      "<class 'int'>\n",
      "<class 'int'>\n",
      "<class 'int'>\n",
      "<class 'int'>\n",
      "<class 'int'>\n",
      "<class 'int'>\n",
      "<class 'int'>\n",
      "<class 'int'>\n",
      "<class 'int'>\n",
      "<class 'int'>\n",
      "<class 'int'>\n",
      "<class 'int'>\n",
      "<class 'int'>\n",
      "<class 'int'>\n",
      "<class 'int'>\n",
      "<class 'int'>\n",
      "<class 'int'>\n",
      "<class 'int'>\n",
      "<class 'int'>\n",
      "<class 'int'>\n",
      "<class 'int'>\n",
      "<class 'int'>\n",
      "<class 'int'>\n",
      "<class 'int'>\n",
      "<class 'int'>\n",
      "<class 'int'>\n",
      "<class 'int'>\n",
      "<class 'int'>\n",
      "<class 'int'>\n",
      "<class 'int'>\n",
      "<class 'int'>\n",
      "<class 'int'>\n",
      "<class 'int'>\n",
      "<class 'int'>\n",
      "<class 'int'>\n",
      "<class 'int'>\n",
      "<class 'int'>\n"
     ]
    }
   ],
   "source": [
    "for i in range(0,len(text_word)):\n",
    "    print(text_word[i][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "// test\n",
      "digraph {\n",
      "\tA [label=\"sentiment analysis\"]\n",
      "\tA [label=\"sentiment analysis\"]\n",
      "\tB [label=is]\n",
      "\tL [label=\"\"]\n",
      "\tL [label=\"a challenging\"]\n",
      "\tL [label=\"a challenging subject\"]\n",
      "\tL [label=\"a challenging subject in\"]\n",
      "\tL [label=\"a challenging subject in ml\"]\n",
      "\tL [label=\"a challenging subject in ml\"]\n",
      "}\n",
      "8\n",
      "1\n",
      "2\n",
      "// test\n",
      "digraph {\n",
      "\tA [label=\"sentiment analysis\"]\n",
      "\tA [label=\"sentiment analysis\"]\n",
      "\tB [label=is]\n",
      "\tL [label=\"\"]\n",
      "\tL [label=\"a challenging\"]\n",
      "\tL [label=\"a challenging subject\"]\n",
      "\tL [label=\"a challenging subject in\"]\n",
      "\tL [label=\"a challenging subject in ml\"]\n",
      "\tL [label=\"a challenging subject in ml\"]\n",
      "\tA [label=\"\"]\n",
      "\tA [label=\"\"]\n",
      "\tB [label=express]\n",
      "\tL [label=\"\"]\n",
      "\tL [label=\"their emotions\"]\n",
      "\tL [label=\"their emotions in\"]\n",
      "\tL [label=\"their emotions in language\"]\n",
      "\tL [label=\"their emotions in language who\"]\n",
      "\tL [label=\"their emotions in language who\"]\n",
      "\tB [label=is]\n",
      "}\n",
      "41\n"
     ]
    }
   ],
   "source": [
    "count=0\n",
    "temp=''\n",
    "dot = Digraph(comment='test')\n",
    "for i in range(0,len(text_word)):\n",
    "    if(text_word[i][1]==\"VBZ\" or text_word[i][1]==\"VBP\"):\n",
    "        dot.node('B', text_word[i][0])      \n",
    "        count=count+1  \n",
    "        print(count)\n",
    "        temp=''\n",
    "    elif(text_word[i][1]=='.'):\n",
    "        print(dot.source)\n",
    "        print(i)\n",
    "        temp=''\n",
    "        count=0\n",
    "    else:\n",
    "        if(temp==''):\n",
    "            if(text_word[i+1][1]!=\"VBZ\" and text_word[i+1][1]!=\"VBP\" and text_word[i+1][1]!='.'):\n",
    "                temp=text_word[i][0]+' ' +text_word[i+1][0]\n",
    "            else:\n",
    "                tmep=text_word[i][0]\n",
    "        else:\n",
    "            if(text_word[i+1][1]!=\"VBZ\" and text_word[i+1][1]!=\"VBP\" and text_word[i+1][1]!='.'):\n",
    "                temp=temp+' '+text_word[i+1][0]\n",
    "    if(count==0):\n",
    "        dot.node('A', temp)\n",
    "    elif(count==1):\n",
    "        dot.node('L', temp) \n",
    "\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'VBZ'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_word[2][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'test-output\\\\round-table.gv.pdf'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dot = Digraph(comment='test')\n",
    "dot.node('A', 'King Arthur')\n",
    "dot.node('B', 'Sir Bedevere the Wise')\n",
    "dot.node('L', 'Sir Lancelot the Brave')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'test-output\\\\round-table.gv.pdf'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dot.render('test-output/round-table.gv', view=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "# Natural Language Toolkit: Phrase Extraction Algorithm\n",
    "#\n",
    "# Copyright (C) 2001-2019 NLTK Project\n",
    "# Authors: Liling Tan, Fredrik Hedman, Petra Barancikova\n",
    "# URL: <http://nltk.org/>\n",
    "# For license information, see LICENSE.TXT\n",
    "\n",
    "\n",
    "def extract(\n",
    "    f_start,\n",
    "    f_end,\n",
    "    e_start,\n",
    "    e_end,\n",
    "    alignment,\n",
    "    f_aligned,\n",
    "    srctext,\n",
    "    trgtext,\n",
    "    srclen,\n",
    "    trglen,\n",
    "    max_phrase_length,\n",
    "):\n",
    "    \"\"\"\n",
    "    This function checks for alignment point consistency and extracts\n",
    "    phrases using the chunk of consistent phrases.\n",
    "\n",
    "    A phrase pair (e, f ) is consistent with an alignment A if and only if:\n",
    "\n",
    "    (i) No English words in the phrase pair are aligned to words outside it.\n",
    "\n",
    "           ∀e i ∈ e, (e i , f j ) ∈ A ⇒ f j ∈ f\n",
    "\n",
    "    (ii) No Foreign words in the phrase pair are aligned to words outside it.\n",
    "\n",
    "            ∀f j ∈ f , (e i , f j ) ∈ A ⇒ e i ∈ e\n",
    "\n",
    "    (iii) The phrase pair contains at least one alignment point.\n",
    "\n",
    "            ∃e i ∈ e  ̄ , f j ∈ f  ̄ s.t. (e i , f j ) ∈ A\n",
    "\n",
    "    :type f_start: int\n",
    "    :param f_start: Starting index of the possible foreign language phrases\n",
    "    :type f_end: int\n",
    "    :param f_end: Starting index of the possible foreign language phrases\n",
    "    :type e_start: int\n",
    "    :param e_start: Starting index of the possible source language phrases\n",
    "    :type e_end: int\n",
    "    :param e_end: Starting index of the possible source language phrases\n",
    "    :type srctext: list\n",
    "    :param srctext: The source language tokens, a list of string.\n",
    "    :type trgtext: list\n",
    "    :param trgtext: The target language tokens, a list of string.\n",
    "    :type srclen: int\n",
    "    :param srclen: The number of tokens in the source language tokens.\n",
    "    :type trglen: int\n",
    "    :param trglen: The number of tokens in the target language tokens.\n",
    "    \"\"\"\n",
    "\n",
    "    if f_end < 0:  # 0-based indexing.\n",
    "        return {}\n",
    "    # Check if alignment points are consistent.\n",
    "    for e, f in alignment:\n",
    "        if (f_start <= f <= f_end) and (e < e_start or e > e_end):\n",
    "            return {}\n",
    "\n",
    "    # Add phrase pairs (incl. additional unaligned f)\n",
    "    phrases = set()\n",
    "    fs = f_start\n",
    "    while True:\n",
    "        fe = min(f_end, f_start + max_phrase_length - 1)\n",
    "        while True:\n",
    "            # add phrase pair ([e_start, e_end], [fs, fe]) to set E\n",
    "            # Need to +1 in range  to include the end-point.\n",
    "            src_phrase = \" \".join(srctext[e_start : e_end + 1])\n",
    "            trg_phrase = \" \".join(trgtext[fs : fe + 1])\n",
    "            # Include more data for later ordering.\n",
    "            phrases.add(\n",
    "                ((e_start, e_end + 1), (f_start, f_end + 1), src_phrase, trg_phrase)\n",
    "            )\n",
    "            fe += 1\n",
    "            if fe in f_aligned or fe >= trglen:\n",
    "                break\n",
    "        fs -= 1\n",
    "        if fs in f_aligned or fs < 0:\n",
    "            break\n",
    "    return phrases\n",
    "\n",
    "def phrase_extraction(srctext, trgtext, alignment, max_phrase_length=0):\n",
    "    \"\"\"\n",
    "    Phrase extraction algorithm extracts all consistent phrase pairs from\n",
    "    a word-aligned sentence pair.\n",
    "\n",
    "    The idea is to loop over all possible source language (e) phrases and find\n",
    "    the minimal foreign phrase (f) that matches each of them. Matching is done\n",
    "    by identifying all alignment points for the source phrase and finding the\n",
    "    shortest foreign phrase that includes all the foreign counterparts for the\n",
    "    source words.\n",
    "\n",
    "    In short, a phrase alignment has to\n",
    "    (a) contain all alignment points for all covered words\n",
    "    (b) contain at least one alignment point\n",
    "\n",
    "    >>> srctext = \"michael assumes that he will stay in the house\"\n",
    "    >>> trgtext = \"michael geht davon aus , dass er im haus bleibt\"\n",
    "    >>> alignment = [(0,0), (1,1), (1,2), (1,3), (2,5), (3,6), (4,9),\n",
    "    ... (5,9), (6,7), (7,7), (8,8)]\n",
    "    >>> phrases = phrase_extraction(srctext, trgtext, alignment)\n",
    "    >>> for i in sorted(phrases):\n",
    "    ...    print(i)\n",
    "    ...\n",
    "    ((0, 1), (0, 1), 'michael', 'michael')\n",
    "    ((0, 2), (0, 4), 'michael assumes', 'michael geht davon aus')\n",
    "    ((0, 2), (0, 4), 'michael assumes', 'michael geht davon aus ,')\n",
    "    ((0, 3), (0, 6), 'michael assumes that', 'michael geht davon aus , dass')\n",
    "    ((0, 4), (0, 7), 'michael assumes that he', 'michael geht davon aus , dass er')\n",
    "    ((0, 9), (0, 10), 'michael assumes that he will stay in the house', 'michael geht davon aus , dass er im haus bleibt')\n",
    "    ((1, 2), (1, 4), 'assumes', 'geht davon aus')\n",
    "    ((1, 2), (1, 4), 'assumes', 'geht davon aus ,')\n",
    "    ((1, 3), (1, 6), 'assumes that', 'geht davon aus , dass')\n",
    "    ((1, 4), (1, 7), 'assumes that he', 'geht davon aus , dass er')\n",
    "    ((1, 9), (1, 10), 'assumes that he will stay in the house', 'geht davon aus , dass er im haus bleibt')\n",
    "    ((2, 3), (5, 6), 'that', ', dass')\n",
    "    ((2, 3), (5, 6), 'that', 'dass')\n",
    "    ((2, 4), (5, 7), 'that he', ', dass er')\n",
    "    ((2, 4), (5, 7), 'that he', 'dass er')\n",
    "    ((2, 9), (5, 10), 'that he will stay in the house', ', dass er im haus bleibt')\n",
    "    ((2, 9), (5, 10), 'that he will stay in the house', 'dass er im haus bleibt')\n",
    "    ((3, 4), (6, 7), 'he', 'er')\n",
    "    ((3, 9), (6, 10), 'he will stay in the house', 'er im haus bleibt')\n",
    "    ((4, 6), (9, 10), 'will stay', 'bleibt')\n",
    "    ((4, 9), (7, 10), 'will stay in the house', 'im haus bleibt')\n",
    "    ((6, 8), (7, 8), 'in the', 'im')\n",
    "    ((6, 9), (7, 9), 'in the house', 'im haus')\n",
    "    ((8, 9), (8, 9), 'house', 'haus')\n",
    "\n",
    "    :type srctext: str\n",
    "    :param srctext: The sentence string from the source language.\n",
    "    :type trgtext: str\n",
    "    :param trgtext: The sentence string from the target language.\n",
    "    :type alignment: list(tuple)\n",
    "    :param alignment: The word alignment outputs as list of tuples, where\n",
    "        the first elements of tuples are the source words' indices and\n",
    "        second elements are the target words' indices. This is also the output\n",
    "        format of nltk.translate.ibm1\n",
    "    :rtype: list(tuple)\n",
    "    :return: A list of tuples, each element in a list is a phrase and each\n",
    "        phrase is a tuple made up of (i) its source location, (ii) its target\n",
    "        location, (iii) the source phrase and (iii) the target phrase. The phrase\n",
    "        list of tuples represents all the possible phrases extracted from the\n",
    "        word alignments.\n",
    "    :type max_phrase_length: int\n",
    "    :param max_phrase_length: maximal phrase length, if 0 or not specified\n",
    "        it is set to a length of the longer sentence (srctext or trgtext).\n",
    "    \"\"\"\n",
    "\n",
    "    srctext = srctext.split()  # e\n",
    "    trgtext = trgtext.split()  # f\n",
    "    srclen = len(srctext)  # len(e)\n",
    "    trglen = len(trgtext)  # len(f)\n",
    "    # Keeps an index of which source/target words that are aligned.\n",
    "    f_aligned = [j for _, j in alignment]\n",
    "    max_phrase_length = max_phrase_length or max(srclen, trglen)\n",
    "\n",
    "    # set of phrase pairs BP\n",
    "    bp = set()\n",
    "\n",
    "    for e_start in range(srclen):\n",
    "        max_idx = min(srclen, e_start + max_phrase_length)\n",
    "        for e_end in range(e_start, max_idx):\n",
    "            # // find the minimally matching foreign phrase\n",
    "            # (f start , f end ) = ( length(f), 0 )\n",
    "            # f_start ∈ [0, len(f) - 1]; f_end ∈ [0, len(f) - 1]\n",
    "            f_start, f_end = trglen - 1, -1  #  0-based indexing\n",
    "\n",
    "            for e, f in alignment:\n",
    "                if e_start <= e <= e_end:\n",
    "                    f_start = min(f, f_start)\n",
    "                    f_end = max(f, f_end)\n",
    "            # add extract (f start , f end , e start , e end ) to set BP\n",
    "            phrases = extract(\n",
    "                f_start,\n",
    "                f_end,\n",
    "                e_start,\n",
    "                e_end,\n",
    "                alignment,\n",
    "                f_aligned,\n",
    "                srctext,\n",
    "                trgtext,\n",
    "                srclen,\n",
    "                trglen,\n",
    "                max_phrase_length,\n",
    "            )\n",
    "            if phrases:\n",
    "                bp.update(phrases)\n",
    "    return bp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "((0, 1), (0, 1), 'michael', 'michael')\n",
      "((0, 2), (0, 4), 'michael assumes', 'michael geht davon aus')\n",
      "((0, 2), (0, 4), 'michael assumes', 'michael geht davon aus ,')\n",
      "((0, 3), (0, 6), 'michael assumes that', 'michael geht davon aus , dass')\n",
      "((0, 4), (0, 7), 'michael assumes that he', 'michael geht davon aus , dass er')\n",
      "((0, 9), (0, 10), 'michael assumes that he will stay in the house', 'michael geht davon aus , dass er im haus bleibt')\n",
      "((1, 2), (1, 4), 'assumes', 'geht davon aus')\n",
      "((1, 2), (1, 4), 'assumes', 'geht davon aus ,')\n",
      "((1, 3), (1, 6), 'assumes that', 'geht davon aus , dass')\n",
      "((1, 4), (1, 7), 'assumes that he', 'geht davon aus , dass er')\n",
      "((1, 9), (1, 10), 'assumes that he will stay in the house', 'geht davon aus , dass er im haus bleibt')\n",
      "((2, 3), (5, 6), 'that', ', dass')\n",
      "((2, 3), (5, 6), 'that', 'dass')\n",
      "((2, 4), (5, 7), 'that he', ', dass er')\n",
      "((2, 4), (5, 7), 'that he', 'dass er')\n",
      "((2, 9), (5, 10), 'that he will stay in the house', ', dass er im haus bleibt')\n",
      "((2, 9), (5, 10), 'that he will stay in the house', 'dass er im haus bleibt')\n",
      "((3, 4), (6, 7), 'he', 'er')\n",
      "((3, 9), (6, 10), 'he will stay in the house', 'er im haus bleibt')\n",
      "((4, 6), (9, 10), 'will stay', 'bleibt')\n",
      "((4, 9), (7, 10), 'will stay in the house', 'im haus bleibt')\n",
      "((6, 8), (7, 8), 'in the', 'im')\n",
      "((6, 9), (7, 9), 'in the house', 'im haus')\n",
      "((8, 9), (8, 9), 'house', 'haus')\n"
     ]
    }
   ],
   "source": [
    "from nltk.translate import PhraseTable\n",
    "srctext = \"michael assumes that he will stay in the house\"\n",
    "trgtext = \"michael geht davon aus , dass er im haus bleibt\"\n",
    "alignment = [(0,0), (1,1), (1,2), (1,3), (2,5), (3,6), (4,9),(5,9), (6,7), (7,7), (8,8)]\n",
    "phrases = phrase_extraction(srctext, trgtext, alignment)\n",
    "for i in sorted(phrases):\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第 1 張\n",
      "第 2 張\n",
      "第 3 張\n",
      "第 4 張\n",
      "第 5 張\n",
      "End\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import urllib.request\n",
    "from bs4 import BeautifulSoup\n",
    "import os\n",
    "import time\n",
    "q='川普'\n",
    "url = 'https://www.google.com/search?q='+q+'&rlz=1C2CAFB_enTW617TW617&source=lnms&tbm=isch&sa=X&ved=0ahUKEwictOnTmYDcAhXGV7wKHX-OApwQ_AUICigB&biw=1128&bih=960'\n",
    "limit = 5\n",
    "header = {'User-Agent': 'Mozilla/5.0'}\n",
    "\n",
    "response = requests.get(url,headers = header) #使用header避免訪問受到限制\n",
    "\n",
    "soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "imgs = soup.find_all('img')\n",
    "folder_path ='./photo/'\n",
    "if (os.path.exists(folder_path) == False): #判斷資料夾是否存在\n",
    "    os.makedirs(folder_path) #Create folder\n",
    "for i , item in enumerate (imgs):\n",
    "    if (item and i < limit ):\n",
    "        src = requests.get(item.get('src')) # use 'get' to get photo link path , requests = send request\n",
    "        img_name = folder_path + str(i + 1) + '.png'\n",
    "        with open(img_name,'wb') as file: #以byte的形式將圖片數據寫入\n",
    "            file.write(src.content)\n",
    "            file.flush()\n",
    "        file.close() #close file\n",
    "        print('第 %d 張' % (i + 1))\n",
    "        #time.sleep(1)\n",
    "\n",
    "print('End')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Item no.: 1 --> Item name = \\u5ddd\\u666e\n",
      "Evaluating...\n",
      "Starting Download...\n",
      "Completed Image ====> 1.phpILH8CP.jpg\n",
      "Completed Image ====> 2.trump.jpg\n",
      "Completed Image ====> 3.phphZ0MGH.jpg\n",
      "Completed Image ====> 4.B10A00_P_02_02.jpg\n",
      "Completed Image ====> 5.A01A00_P_04_02.jpg\n",
      "Completed Image ====> 6.20190925-100917_U1085_M553144_a9c6.jpg\n",
      "Completed Image ====> 7.20191220004521.jpg\n",
      "Completed Image ====> 8.20190924-020704_U1085_M552919_f558.jpg\n",
      "Completed Image ====> 9.2048x1365_876716532259.jpg\n",
      "Invalid or missing image format. Skipping...\n",
      "Completed Image ====> 10.4096x2287_176950641601.jpg\n",
      "\n",
      "Errors: 1\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from google_images_download import google_images_download\n",
    "response = google_images_download.googleimagesdownload()\n",
    "arguments = {\"keywords\":\"川普\",\"limit\":10,\"print_urls\":False}\n",
    "paths = response.download(arguments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
